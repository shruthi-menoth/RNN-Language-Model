{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import  collections\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pickle\n",
    "import reader\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying word ids and sentences for the first 10 samples in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"consumers may want to move their telephones a little closer to the tv set\" [1132, 93, 358, 5, 329, 51, 9836, 6, 326, 2476, 5, 0, 662, 388]\n",
      "\n",
      "\"<unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk>\" [1, 1, 2974, 2158, 9, 381, 1068, 2347, 89, 99, 847, 198, 1, 11, 0, 3383, 1119, 7, 3, 72, 20, 211, 346, 36, 258, 1, 1]\n",
      "\n",
      "\"two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues\" [75, 422, 195, 3917, 4, 249, 1795, 1, 580, 3528, 892, 2374, 6, 3, 297, 11, 2709, 16, 1186, 1, 250]\n",
      "\n",
      "\"and the new syndicated reality show hard copy records viewers ' opinions for possible airing on the next day 's show\" [8, 0, 35, 9922, 3747, 464, 710, 2998, 2037, 3917, 134, 6145, 11, 494, 5894, 16, 0, 130, 272, 9, 464]\n",
      "\n",
      "\"interactive telephone technology has taken a new leap in <unk> and television programmers are racing to exploit the possibilities\" [9958, 732, 503, 30, 641, 6, 35, 6498, 7, 1, 8, 761, 9967, 26, 6587, 5, 6415, 0, 6574]\n",
      "\n",
      "\"eventually viewers may grow <unk> with the technology and <unk> the cost\" [1413, 3917, 93, 1552, 1, 22, 0, 503, 8, 1, 0, 361]\n",
      "\n",
      "\"but right now programmers are figuring that viewers who are busy dialing up a range of services may put down their <unk> control <unk> and stay <unk>\" [29, 382, 99, 9967, 26, 7428, 10, 3917, 56, 26, 3248, 8846, 52, 6, 880, 4, 323, 93, 335, 118, 51, 1, 350, 1, 8, 1337, 1]\n",
      "\n",
      "\"we 've been spending a lot of time in los angeles talking to tv production people says mike parks president of call interactive which supplied technology for both abc sports and nbc 's consumer minutes\" [64, 573, 58, 508, 6, 581, 4, 103, 7, 639, 747, 1921, 5, 662, 359, 108, 44, 5458, 6149, 70, 4, 786, 9958, 41, 7746, 503, 11, 179, 2158, 1259, 8, 1795, 9, 580, 1495]\n",
      "\n",
      "\"with the competitiveness of the television market these days everyone is looking for a way to get viewers more excited\" [22, 0, 9643, 4, 0, 761, 47, 144, 171, 1376, 13, 735, 11, 6, 229, 5, 188, 3917, 45, 9684]\n",
      "\n",
      "\"one of the leaders behind the expanded use of N numbers is call interactive a joint venture of giants american express co. and american telephone & telegraph co\" [54, 4, 0, 815, 1116, 0, 2439, 269, 4, 3, 1619, 13, 786, 9958, 6, 795, 818, 4, 2172, 140, 1021, 95, 8, 140, 732, 82, 3133, 570]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import reader\n",
    "\n",
    "v = reader._read_words(\"data/ptb.valid.txt\")\n",
    "\n",
    "train_data, valid_data, test_data, vocabulary = reader.ptb_raw_data(\"data/\")\n",
    "\n",
    "x, y = reader.ptb_producer(valid_data,10,5)\n",
    "\n",
    "count = 0\n",
    "word = 0\n",
    "sentence = ''\n",
    "ids = []\n",
    "# First 10 sentences\n",
    "while count<10:\n",
    "    end = v[word]\n",
    "    word_id = valid_data[word]\n",
    "    if end == '<eos>':\n",
    "        count += 1\n",
    "        print('\"'+sentence.strip()+'\" '+str(ids)+'\\n')\n",
    "        sentence = ''\n",
    "        ids = []\n",
    "    else:\n",
    "        sentence += end + ' '\n",
    "        ids.append(word_id)\n",
    "    word += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  929589\n",
      "val:  73760\n",
      "test:  82430\n"
     ]
    }
   ],
   "source": [
    "print('train: ',len(train_data))\n",
    "print('val: ', len(valid_data))\n",
    "print('test: ', len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionaries\n",
    "import reader\n",
    "train_data, valid_data, test_data, word_to_id = reader.ptb_raw_data(\"data/\")\n",
    "with open('word_to_id.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_to_id, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  config(object):\n",
    "    vocab_size = vocab_size\n",
    "    batch_size = 20\n",
    "    num_steps = 20  # sequence length\n",
    "    hidden_size = 200  # number of hidden units in LSTM\n",
    "    keep_prob = 0.5  # 1 - dropoff rate\n",
    "    num_layers = 2  # number of LSTM layers\n",
    "    max_grad_norm = 5  # max gradient \n",
    "    init_scale = 0.1  # the initial scale of the weights\n",
    "    max_epoch = 4  # the number of epochs trained with the initial learning rate\n",
    "    max_max_epoch = 13  # the total number of epochs for training\n",
    "    learning_rate = 1.0  # the initial value of the learning rate\n",
    "    lr_decay = 0.5  # the decay of the learning rate for each epoch after \"max_epoch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config.batch_size\n",
    "num_steps = config.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(887521, 44376)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = train_data\n",
    "\n",
    "data_len = len(raw_data)\n",
    "batch_len = data_len // batch_size\n",
    "\n",
    "data_len, batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 44376)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(raw_data[0:batch_size * batch_len]).reshape(batch_size, batch_len)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2218"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_size  = (batch_len - 1) // num_steps\n",
    "epoch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 20), (20, 20))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i  =  0\n",
    "\n",
    "x  =  tf.strided_slice(data, [0, i * num_steps], [batch_size, (i + 1) * num_steps])\n",
    "y = tf.strided_slice(data, [0, i * num_steps + 1], [batch_size, (i + 1) * num_steps + 1])\n",
    "\n",
    "x = x.eval()\n",
    "y = y.eval()\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(x, data[0:batch_size:1, i * num_steps:(i + 1) * num_steps:1])\n",
    "assert np.array_equal(y, data[0:batch_size:1, i * num_steps + 1:(i + 1) * num_steps + 1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBInput(object):\n",
    "    \"\"\"The input data.\"\"\"\n",
    "\n",
    "    def __init__(self, config, data, name=None):\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        \n",
    "     \n",
    "        self.input_data, self.targets = reader.ptb_producer(\n",
    "            data, batch_size, num_steps, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    \"\"\"The PTB model.\"\"\"\n",
    "\n",
    "    def __init__(self, is_training, config, input_=None):\n",
    "        batch_size = config.batch_size\n",
    "        num_steps = config.num_steps\n",
    "        hidden_size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        if input_ is not None:\n",
    "            # For normal training and validation\n",
    "            self._input = input_\n",
    "            self._input_data = input_.input_data\n",
    "            self._targets = input_.targets\n",
    "            \n",
    "        else:\n",
    "            # For text generations\n",
    "            self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "            self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        def lstm_cell():\n",
    "          \n",
    "            \n",
    "            if 'reuse' in inspect.getargspec(\n",
    "                    tf.contrib.rnn.BasicLSTMCell.__init__).args:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True,\n",
    "                    reuse=tf.get_variable_scope().reuse)\n",
    "            else:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True)\n",
    "            \n",
    "         \n",
    "        attn_cell = lstm_cell\n",
    "\n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "\n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(\n",
    "                    lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "\n",
    "       \n",
    "        attn_cells = [attn_cell() for _ in range(config.num_layers)]\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell(attn_cells, state_is_tuple=True)\n",
    "        \n",
    "  \n",
    "         \n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "       \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, hidden_size], dtype=tf.float32)\n",
    "            input_embeddings = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "       \n",
    "        if is_training and config.keep_prob < 1:\n",
    "            input_embeddings = tf.nn.dropout(input_embeddings, config.keep_prob)\n",
    "\n",
    "    \n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        \n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                (cell_output, state) = stacked_lstm(input_embeddings[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "            \n",
    "        output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, hidden_size])\n",
    "        \n",
    "        # Compute logits\n",
    "        softmax_w = tf.get_variable(\n",
    "            \"softmax_w\", [hidden_size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\n",
    "            \"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "        \n",
    "        self._logits = logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "       \n",
    "        self._logits_sample = tf.multinomial(logits, 1)\n",
    "        \n",
    "        # Instead of sampling, return the position with the largest logit\n",
    "        self._logits_max = tf.argmax(logits, 1)\n",
    "        \n",
    "        # Reshape logits to be 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "\n",
    "      \n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,  # shape: [batch_size, num_steps, vocab_size]\n",
    "            self._targets,  # shape: [batch_size, num_steps]\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "\n",
    "        # Update the cost variables\n",
    "        self._cost = cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # Optimizer\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "        \n",
    "        \n",
    "    # To update learning rate\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "    \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    \n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def logits_sample(self):\n",
    "        return self._logits_sample\n",
    "    \n",
    "    @property\n",
    "    def logits_max(self):\n",
    "        return self._logits_max\n",
    "    \n",
    "    @property\n",
    "    def logits(self):\n",
    "        return self._logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"cost\": model.cost,\n",
    "        \"final_state\": model.final_state,\n",
    "    }\n",
    "    \n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op\n",
    "    \n",
    "   \n",
    "    for step in range(model.input.epoch_size):\n",
    "        feed_dict = {}\n",
    "        \n",
    "      \n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        \n",
    "     \n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "        \n",
    "      \n",
    "        costs += cost\n",
    "        iters += model.input.num_steps\n",
    "\n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "            print(\"%.3f (raw step: %.0f) perplexity: %.3f speed: %.0f wps\" %\n",
    "                  (step * 1.0 / model.input.epoch_size,\n",
    "                   step,\n",
    "                   np.exp(costs / iters),\n",
    "                   iters * model.input.batch_size / (time.time() - start_time)))\n",
    "            \n",
    "\n",
    "    return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = np.array(word_to_id['a']).reshape(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(session, model, feed, text_length):\n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"final_state\": model.final_state,\n",
    "        \"logits\": model.logits_sample\n",
    "    }\n",
    "    \n",
    "    generated_text = [feed]\n",
    "    \n",
    "    for i in range(text_length):\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.input_data] = feed\n",
    "        \n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "        \n",
    "\n",
    "        state = vals[\"final_state\"]\n",
    "        feed = vals[\"logits\"]\n",
    "        \n",
    "    \n",
    "        generated_text.append(feed)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'model_output' + '_' + datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "save_path2 = 'assignment2/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_config = config()\n",
    "\n",
    "eval_config = config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\Anaconda3\\envs\\cs231n\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-d88b96248978>:119: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "WARNING:tensorflow:From <ipython-input-25-7531a70ccf14>:29: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path model_output_2018-05-29-21-49\\model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Model/global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 0.\n",
      "INFO:tensorflow:Recording summary at step 1295.\n",
      "INFO:tensorflow:Model/global_step/sec: 11.0151\n",
      "INFO:tensorflow:Recording summary at step 2531.\n",
      "INFO:tensorflow:Model/global_step/sec: 10.3341\n",
      "INFO:tensorflow:Recording summary at step 4210.\n",
      "INFO:tensorflow:Model/global_step/sec: 13.9815\n",
      "INFO:tensorflow:Recording summary at step 5806.\n",
      "INFO:tensorflow:Model/global_step/sec: 13.2597\n",
      "INFO:tensorflow:Saving checkpoint to path model_output_2018-05-29-21-49\\model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 7385.\n",
      "INFO:tensorflow:Model/global_step/sec: 13.1582\n",
      "INFO:tensorflow:Recording summary at step 8839.\n",
      "INFO:tensorflow:Model/global_step/sec: 12.1087\n",
      "INFO:tensorflow:Recording summary at step 10103.\n",
      "INFO:tensorflow:Model/global_step/sec: 10.5738\n",
      "INFO:tensorflow:Recording summary at step 11434.\n",
      "INFO:tensorflow:Model/global_step/sec: 11.0672\n",
      "INFO:tensorflow:Recording summary at step 12531.\n",
      "INFO:tensorflow:Model/global_step/sec: 9.13428\n",
      "INFO:tensorflow:Saving checkpoint to path model_output_2018-05-29-21-49\\model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 13971.\n",
      "INFO:tensorflow:Recording summary at step 15656.\n",
      "INFO:tensorflow:Recording summary at step 17382.\n",
      "INFO:tensorflow:Recording summary at step 19241.\n",
      "INFO:tensorflow:Recording summary at step 21029.\n",
      "INFO:tensorflow:Saving checkpoint to path model_output_2018-05-29-21-49\\model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 22818.\n",
      "INFO:tensorflow:Recording summary at step 24328.\n",
      "INFO:tensorflow:Recording summary at step 25800.\n",
      "INFO:tensorflow:Recording summary at step 27492.\n",
      "INFO:tensorflow:Recording summary at step 28834.\n",
      "INFO:tensorflow:Saving checkpoint to path model_output_2018-05-29-21-49\\model.ckpt\n",
      "INFO:tensorflow:Recording summary at step 28834.\n"
     ]
    }
   ],
   "source": [
    "orig_stdout = sys.stdout\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = PTBInput(config=train_valid_config, data=train_data, name=\"TrainInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "        tf.summary.scalar(\"Training_Loss\", m.cost)\n",
    "        tf.summary.scalar(\"Learning_Rate\", m.lr)\n",
    "\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = PTBInput(config=train_valid_config, data=valid_data, name=\"ValidInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n",
    "        tf.summary.scalar(\"Validation_Loss\", mvalid.cost)\n",
    "    \n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = PTBInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mtest = PTBModel(is_training=False, config=eval_config, input_=test_input)\n",
    "    \n",
    "\n",
    "    with tf.name_scope(\"Feed\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mfeed = PTBModel(is_training=False, config=eval_config)\n",
    "            \n",
    "    sv = tf.train.Supervisor(logdir=save_path)\n",
    "    with sv.managed_session() as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            \n",
    "            log_file_path = 'log_file_' + str(i) + '.txt'\n",
    "            f = open(os.path.join(save_path, log_file_path), 'w')\n",
    "            sys.stdout = f\n",
    "            \n",
    "            \n",
    "            lr_decay = config.lr_decay**max(i + 1 - config.max_epoch, 0.0)\n",
    "            m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            \n",
    "   \n",
    "            train_perplexity = run_epoch(session, m, eval_op=m.train_op, verbose=True)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "            \n",
    "            valid_perplexity = run_epoch(session, mvalid)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "            \n",
    "           \n",
    "            generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "            generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "            print(\"Sample text generation:\", generated_text)\n",
    "            \n",
    "            f.close()\n",
    "            \n",
    "        \n",
    "        log_file_path = 'log_file_test_perplexity.txt'\n",
    "        f = open(os.path.join(save_path, log_file_path), 'w')\n",
    "        sys.stdout = f\n",
    "\n",
    "        test_perplexity = run_epoch(session, mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "        \n",
    "        print(\"Saving model to %s.\" % save_path2)\n",
    "        sv.saver.save(session, save_path2, global_step=sv.global_step)\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "\n",
    "sys.stdout = orig_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting training and validation losses with respect to epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVAAAAFACAYAAADqPiRCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl0FfX9//HnOzeBEMISCGAgKGCiKBASCIqlFcWl7lttRcWtWqvVWq1a1LZqF1tbUdG2WnErVapF1GqVukOtv6+KBBBBUGQRwhqWBAIEQvL5/TGTECDLzc2d3CT39TjnnnvvzNyZN6CvM8tn3mPOOUREpPESYl2AiEhrpQAVEYmQAlREJEIKUBGRCClARUQipAAVEYmQAlREJEIKUBGRCClARUQilBjrApoiPT3d9evXL9ZliEgbU1BQsNE516Oh5Vp1gPbr14/Zs2fHugwRaWPM7OtwltMhvIhIhBSgIiIRUoCKiESoVZ8DFWmJysvLKSwspKysLNalSAOSk5PJzMwkKSkpot8rQEWirLCwkE6dOtGvXz/MLNblSB2cc2zatInCwkL69+8f0Tp0CC8SZWVlZXTv3l3h2cKZGd27d2/SkYICVCQACs/Woan/TgpQEZEIKUBF2pji4mIeeeSRiH572mmnUVxcXO8yd955J++8805E699fv3792LhxY1TWFQtxFaBTp8J778W6CpFg1RegFRUV9f52+vTpdO3atd5lfv3rX3PiiSdGXF9bElcBescd8Nhjsa5CJFi33XYbS5cuJTc3l1tvvZWZM2dy/PHHc9FFFzFkyBAAzjnnHIYPH86gQYOYNGlS9W+r9ghXrFjBEUccwQ9+8AMGDRrEySefzM6dOwG4/PLLmTZtWvXyd911F8OGDWPIkCEsXrwYgKKiIk466SSGDRvGD3/4Qw455JAG9zQfeOABBg8ezODBg5k4cSIA27dv5/TTT2fo0KEMHjyYf/7zn9V/xiOPPJKcnBxuueWW6P4FNkJcDWPKyoIlS2JdhcSTG2+EefOiu87cXPDzpVb33nsvCxYsYJ6/4ZkzZzJr1iwWLFhQPVznqaeeolu3buzcuZMRI0bwne98h+7du++zniVLlvDcc8/x+OOP873vfY8XX3yRcePGHbC99PR05syZwyOPPMKECRN44okn+NWvfsWYMWO4/fbbeeONN/YJ6doUFBTw9NNP8/HHH+Oc4+ijj2b06NEsW7aM3r178/rrrwNQUlLC5s2befnll1m8eDFm1uAphyDF1R5odrYXoM7FuhKR5nXUUUftM9bx4YcfZujQoYwcOZJVq1axpJY9i/79+5ObmwvA8OHDWbFiRa3rPu+88w5Y5oMPPmDs2LEAnHLKKaSlpdVb3wcffMC5555Lx44dSU1N5bzzzuN///sfQ4YM4Z133mH8+PH873//o0uXLnTu3Jnk5GSuuuoqXnrpJVJSUhr71xE1cbUHmp0NpaWwYQP06hXraiQe1Len2Jw6duxY/XnmzJm88847fPjhh6SkpHDcccfVOhayffv21Z9DoVD1IXxdy4VCIfbs2QN4g9Qbo67lDzvsMAoKCpg+fTq33347J598MnfeeSezZs3i3Xff5fnnn+fPf/4z78Xo4kZc7YFmZXnvOoyXtqxTp05s27atzvklJSWkpaWRkpLC4sWL+eijj6Jewze/+U2mTp0KwFtvvcWWLVvqXf7YY4/lX//6Fzt27GD79u28/PLLfOtb32LNmjWkpKQwbtw4brnlFubMmUNpaSklJSWcdtppTJw4sfpURSzE3R4oeAH6zW/GthaRoHTv3p1Ro0YxePBgTj31VE4//fR95p9yyin89a9/JScnh8MPP5yRI0dGvYa77rqLCy+8kH/+85+MHj2ajIwMOnXqVOfyw4YN4/LLL+eoo44C4KqrriIvL48333yTW2+9lYSEBJKSknj00UfZtm0bZ599NmVlZTjnePDBB6Nef7issbvaLUl+fr5rTEPl8nLo0AHGj4d77gmwMIlrixYt4ogjjoh1GTG1a9cuQqEQiYmJfPjhh1x77bUx3VOsT23/XmZW4JzLb+i3cbUHmpQE/frpEF4kaCtXruR73/selZWVtGvXjscffzzWJQUi8AA1sxAwG1jtnDvDzPoDzwPdgDnAJc653WbWHvg7MBzYBFzgnFsR7Xqys+Grr6K9VhGpKTs7m7lz58a6jMA1x0WknwCLanz/A/Cgcy4b2AJc6U+/EtjinMsCHvSXizoNZRKRaAk0QM0sEzgdeML/bsAYYJq/yGTgHP/z2f53/PknWAAtbbKyvKFM69dHe80iEm+C3gOdCPwMqPS/dweKnXN7/O+FQB//cx9gFYA/v8Rffh9mdrWZzTaz2UVFRY0uqOpKvA7jRaSpAgtQMzsD2OCcK6g5uZZFXRjz9k5wbpJzLt85l9+jR4OPbT5AzaFMIiJNEeQe6CjgLDNbgXfRaAzeHmlXM6u6eJUJrPE/FwJ9Afz5XYDN0S7qkEMgFFKAitSUmpoKwJo1azj//PNrXea4446joWGDEydOZMeOHdXfw2mPF467776bCRMmNHk90RZYgDrnbnfOZTrn+gFjgfeccxcDM4Cqf6HLgFf8z6/63/Hnv+cCGKSalAT9++sQXqQ2vXv3ru60FIn9AzSc9nitWSxu5RwP/NTMvsI7x/mkP/1JoLs//afAbUEVUHUlXqQtGj9+/D79QO+++27uv/9+SktLOeGEE6pbz73yyisH/HbFihUMHjwYgJ07dzJ27FhycnK44IIL9rkX/tprryU/P59BgwZx1113AV6DkjVr1nD88cdz/PHHA/s2TK6tXV19bfPqMm/ePEaOHElOTg7nnntu9W2iDz/8cHWLu6pGJv/973/Jzc0lNzeXvLy8em9xjUSzDKR3zs0EZvqflwFH1bJMGfDd5qgnKwvef98byqRH10iQliy5kdLS6N6Bk5qaS3Z23V1Kxo4dy4033siPfvQjAKZOncobb7xBcnIyL7/8Mp07d2bjxo2MHDmSs846q87nAj366KOkpKQwf/585s+fz7Bhw6rn3XPPPXTr1o2KigpOOOEE5s+fzw033MADDzzAjBkzSE9P32dddbWrS0tLC7ttXpVLL72UP/3pT4wePZo777yTX/3qV0ycOJF7772X5cuX0759++rTBhMmTOAvf/kLo0aNorS0lOTk5LD/nsMRV81EqmRnw/btGsokbVNeXh4bNmxgzZo1fPrpp6SlpXHwwQfjnOOOO+4gJyeHE088kdWrV7O+nv8J3n///eogy8nJIScnp3re1KlTGTZsGHl5eSxcuJDPP/+83prqalcH4bfNA68RSnFxMaNHjwbgsssu4/3336+u8eKLL+bZZ58lMdHbNxw1ahQ//elPefjhhykuLq6eHi1xdStnlZpX4g86KLa1SNtW355ikM4//3ymTZvGunXrqg9np0yZQlFREQUFBSQlJdGvX78GH+lb297p8uXLmTBhAp988glpaWlcfvnlDa6nvssZ4bbNa8jrr7/O+++/z6uvvspvfvMbFi5cyG233cbpp5/O9OnTGTlyJO+88w4DBw6MaP21ics9ULW1k7Zu7NixPP/880ybNq36qnpJSQk9e/YkKSmJGTNm8PXXX9e7jmOPPZYpU6YAsGDBAubPnw/A1q1b6dixI126dGH9+vX85z//qf5NXa306mpX11hdunQhLS2teu/1mWeeYfTo0VRWVrJq1SqOP/54/vjHP1JcXExpaSlLly5lyJAhjB8/nvz8/OpHjkRLXO6B9usHiYm6Ei9t16BBg9i2bRt9+vQhIyMDgIsvvpgzzzyT/Px8cnNzG9wTu/baa7niiivIyckhNze3utXc0KFDycvLY9CgQQwYMIBRo0ZV/+bqq6/m1FNPJSMjgxkzZlRPr6tdXX2H63WZPHky11xzDTt27GDAgAE8/fTTVFRUMG7cOEpKSnDOcdNNN9G1a1d++ctfMmPGDEKhEEceeSSnnnpqo7dXn7hqZ1fTYYfB0KHwwgtRLkrintrZtS5NaWcXl4fw4B3Gaw9URJoibgNUXZlEpKniNkCzsryhTOvWxboSaYta86mxeNLUf6e4DVB1ZZKgJCcns2nTJoVoC+ecY9OmTU0aXB+XV+Fh37GgEYymEKlTZmYmhYWFRNJuUZpXcnIymZmZEf8+bgP0kEO8oUwaCyrRlpSURP/+/WNdhjSDuD2ET0xUVyYRaZq4DVBQVyYRaZq4DtCqsaA61y8ikYjrAK3qyqShTCISibgPUNBhvIhEJq4DtKorky4kiUgk4jpANZRJRJoirgM0MREGDFCAikhk4jpAQV2ZRCRycR+g2dkayiQikVGA+kOZ1q6NdSUi0trEfYDqSryIRCruA1RjQUUkUnEfoAcfDElJClARaby4D1B1ZRKRSMV9gIK6MolIZBSgaCiTiERGAYp3JX7HDg1lEpHGUYCiK/EiEhkFKBoLKiKRUYCioUwiEhkFKOrKJCKRUYD61JVJRBpLAerTUCYRaSwFqC872xvKtGZNrCsRkdZCAerTlXgRaSwFqE9jQUWksQILUDNLNrNZZvapmS00s1/50/9mZsvNbJ7/yvWnm5k9bGZfmdl8MxsWVG210VAmEWmsxADXvQsY45wrNbMk4AMz+48/71bn3LT9lj8VyPZfRwOP+u/NIhTyhjLpEF5EwhXYHqjzlPpfk/xXfde4zwb+7v/uI6CrmWUEVV9t1JVJRBoj0HOgZhYys3nABuBt59zH/qx7/MP0B82svT+tD7Cqxs8L/Wn7r/NqM5ttZrOLioqiWm/VUKbKyqiuVkTaqEAD1DlX4ZzLBTKBo8xsMHA7MBAYAXQDxvuLW22rqGWdk5xz+c65/B49ekS13qws2LlTXZlEJDzNchXeOVcMzAROcc6t9Q/TdwFPA0f5ixUCfWv8LBNo1lGZuhIvIo0R5FX4HmbW1f/cATgRWFx1XtPMDDgHWOD/5FXgUv9q/EigxDnXrPuCVQGqC0kiEo4gr8JnAJPNLIQX1FOdc6+Z2Xtm1gPvkH0ecI2//HTgNOArYAdwRYC11apvX2jXTnugIhKewALUOTcfyKtl+pg6lnfAdUHVE46qoUwKUBEJh+5E2k/VlXgRkYYoQPdT1dZOQ5lEpCEK0P1kZ3tDmdSVSUQaogDdj67Ei0i4FKD7qWprpwtJItIQBeh+NJRJRMKlAN2PujKJSLgUoLVQVyYRCYcCtBbqyiQi4VCA1iIrC8rKNJRJROqnAK2FujKJSDgUoLXQWFARCYcCtBaZmRrKJCINU4DWIhSCQw9VgIpI/RSgdVBXJhFpiAK0DurKJCINUYDWITvbG8q0enWsKxGRlkoBWgddiReRhihA66CuTCLSEAVoHfr2hfbtFaAiUjcFaB0SEryhTDqEF5G6KEDrkZWlPVARqZsCtB7Z2bB0qYYyiUjtFKD10FAmEamPArQeuhIvIvVRgNZDY0FFpD4K0HpkZmook4jUTQFaj6qhTApQEamNArQB6sokInVRgDYgK0tDmUSkdgrQBlQNZSosjHUlItLSKEAboCvxIlIXBWgDNBZUROqiAG1AZiYkJytAReRACtAGqCuTiNRFARoGdWUSkdooQMOgrkwiUhsFaBiys2HXLg1lEpF9BRagZpZsZrPM7FMzW2hmv/Kn9zezj81siZn908za+dPb+9+/8uf3C6q2xtKVeBGpTZB7oLuAMc65oUAucIqZjQT+ADzonMsGtgBX+stfCWxxzmUBD/rLtQgaCyoitQksQJ2n1P+a5L8cMAaY5k+fDJzjfz7b/44//wQzs6Dqa4w+fTSUSUQOFOg5UDMLmdk8YAPwNrAUKHbO7fEXKQT6+J/7AKsA/PklQPda1nm1mc02s9lFRUVBll9NXZlEpDaBBqhzrsI5lwtkAkcBR9S2mP9e296mO2CCc5Occ/nOufwePXpEr9gGqCuTiOyvWa7CO+eKgZnASKCrmSX6szKBNf7nQqAvgD+/C7C5OeoLh4Yyicj+grwK38PMuvqfOwAnAouAGcD5/mKXAa/4n1/1v+PPf885d8AeaKxkZXlDmVatinUlItJSJDa8SMQygMlmFsIL6qnOudfM7HPgeTP7LTAXeNJf/kngGTP7Cm/Pc2yAtTVazSvxhxwS21pEpGUILECdc/OBvFqmL8M7H7r/9DLgu0HV01RVAbpkCZxwQmxrEZGWodGH8GaWZmY5QRTTkvXuraFMIrKvsALUzGaaWWcz6wZ8CjxtZg8EW1rLkpDgnQfVlXgRqRLuHmgX59xW4DzgaefccLyLQnElO1t7oCKyV7gBmmhmGcD3gNcCrKdFq3rAXEVFrCsRkZYg3AD9NfAmsNQ594mZDQDibl8sOxt271ZXJhHxhHUV3jn3AvBCje/LgO8EVVRLVbMrk4YyiUi4F5EOM7N3zWyB/z3HzH4RbGktj7oyiUhN4R7CPw7cDpRD9RjPFjXQvTn07g0dOuhCkoh4wg3QFOfcrP2m7al1yTZMXZlEpKZwA3SjmR2K3x3JzM4H1gZWVQumrkwiUiXcWzmvAyYBA81sNbAcGBdYVS1Ydja8/ro3lCkUinU1IhJL4V6FXwacaGYdgQTn3LZgy2q5srK8oUyrVkG/frGuRkRiKdyr8D8xs87ADuBBM5tjZicHW1rLpCvxIlIl3HOg3/dv5TwZ6AlcAdwbWFUtWM2uTCIS38IN0KrHbZyGdy/8p9T+CI42LyNDQ5lExBNugBaY2Vt4AfqmmXUC4vLhFurKJCJVwr0KfyXes92XOed2+G3trgiurJYtOxs+/zzWVYhIrIW7B3oM8IVzrtjMxgG/wHvscFzKyoJly9SVSSTehRugjwI7zGwo8DPga+DvgVXVwlV1ZdID5kTiW7gBusd/QubZwEPOuYeATsGV1bLpSryIQPgBus3MbgcuAV73n7SZFFxZLVtVWztdSBKJb+EG6AXALrzxoOuAPsB9gVXVwvXuDSkp2gMViXdhBagfmlOALmZ2BlDmnIvbc6Bm3l6oAlQkvoV7K+f3gFl4z23/HvCx35EpbmksqIiEOw7058AI59wGADPrAbwDTAuqsJYuOxtee01dmUTiWbjnQBOqwtO3qRG/bZOqhjKtXBnrSkQkVsLdA33DzN4EnvO/XwBMD6ak1qHmlfj+/WNbi4jERrgXkW7Fa6icAwwFJjnnxgdZWEunsaAiEu4eKM65F4EXA6ylVcnI0FAmkXhXb4Ca2Tb85yDtPwtwzrnOgVTVClQNZdKVeJH4VW+AOufazO2alZV7WLZsPCkpA+nd+wdRWWd2NixYEJVViUgrFDdX0hMSEtm69WNWrvw9zkWnjZK6MonEt7gJUIC+fW+irGw5Gze+EpX1ZWdDebmGMonEq7gK0PT0c0hO7kdh4YNRWZ+uxIvEt7gKULMQffrcQEnJB2zdOrvJ61NXJpH4FlcBCpCRcSWhUKeo7IVmZEDHjtoDFYlXcRegiYmdyci4kqKiqezatbpJ66oayrRwYZSKE5FWJe4CFKBPnxtwrpLVq//c5HWddhq8/TZMj+sbW0XiU2ABamZ9zWyGmS0ys4Vm9hN/+t1mttrM5vmv02r85nYz+8rMvjCzbwdVW4cO/UlPP4c1ax6jomJ7k9Z1552QkwNXXAEbNjS8vIi0HUHuge4BbnbOHQGMBK4zsyP9eQ8653L913QAf95YYBBwCvCI/+iQQGRm3sSePVtYt65pfaGTk+Ef/4CSEvj+98HVdt+WiLRJgQWoc26tc26O/3kbsAjvUSB1ORt43jm3yzm3HPgKOCqo+rp0GUWnTvkUFk7EucomrWvQILjvPnj9dXj00SgVKCItXrOcAzWzfkAe8LE/6Xozm29mT5lZmj+tD1DzQcGF1BK4Zna1mc02s9lFRUVNqYnMzJvYufNLNm/+T8TrqXL99XDKKXDzzfD5501enYi0AoEHqJml4nVxutE5txXvGfOHArnAWuD+qkVr+fkBB8TOuUnOuXznXH6PHj2aVFuPHt+lXbs+rFrV9CFNZvD005CaChdfDLt2NXmVItLCBRqgZpaEF55TnHMvATjn1jvnKpx33Pw4ew/TC4G+NX6eCawJsr6EhCQyM39McfG7lJbOb/L6DjoInnoK5s2DX/wiCgWKSIsW5FV4A54EFjnnHqgxPaPGYucCVf2MXgXGmll7M+sPZOM9yC5QGRlXk5CQQmHhxKis78wz4dprYcIEePfdqKxSRFqoIPdARwGXAGP2G7L0RzP7zMzmA8cDNwE45xYCU4HPgTeA61y02ibVIykpjYMOupz166ewe/f6qKxzwgQYOBAuuww2bYrKKkWkBTLXisfd5Ofnu9mzm35P+44dXzJr1uEccshd9O9/d9MLA+bOhaOPhrPOghde8M6RikjrYGYFzrn8hpaLyzuR9peSchjdu5/BmjWPUFFRFpV15uXBPffAiy96F5dEpO1RgPoyM2+ivLyIDRv+EbV13nwzjBkDN9yghiMibZEC1Ne16/F07JjjD6yPzmmNhASYPBnatYNx47zmyyLSdihAfVUD67dv/4wtW6J3+TwzEx57DGbNgl//OmqrFZEWQAFaQ69eF5KU1CtqHeurfPe7XrOR3/0OPvggqqsWkRhSgNaQkNCePn1+xObN09m+fXFU1/3QQ9C/v3coX1IS1VWLSIwoQPfTu/c1mLVn9eqHorreTp3g2WehsBCuuy6qqxaRGFGA7qddu5706jWOdesmU14e3VHwI0fCXXfBlCleCzwRad0UoLXIzLyRysqdrFkzKerrvv12+MY3vNs9V6yI+upFpBkpQGuRmjqYtLSTWL36z1RW7o7quhMTvUN55+CSS6Ai8JtVRSQoCtA6ZGbexO7daygqeiHq6+7fHx55xLsif++9UV+9iDQTBWgdunX7NikpA1m16sGoDayv6eKL4cILvXOiswLvOSUiQVCA1sEsgczMGyktLaCkJPqDN828vdA+fbwwLS2N+iZEJGAK0Hr06nUJiYndoj6wvkrXrvDMM7B0Kdx4YyCbEJEAKUDrEQql0Lv3NWzc+C927lwWyDaOPda7Mv/kk/DSS4FsQkQCogBtQJ8+12GWSGHhw4Ft4+67IT8ffvADWL06sM2ISJQpQBvQvn1veva8gHXrnmTPnmDuwUxK8gbXl5V5Xewrm/aUZRFpJgrQMGRm3kRFRSlr1z4R2DYOO8y7X/7dd+HBYE65ikiUKUDD0KnTMLp0OZbCwoeprNwT2HauvBLOOQfuuMN7sqeItGwK0DBlZt7Erl0r2bjx5cC2YQaPPw7du8O550IUHvckIgFSgIYpPf1MkpMHBDakae924OWXYc8eOOYY+P3vdbunSEulAA2TWYjMzJ+wdeuHbN36caDbOvpomD8fzjvPO5w//ng1HhFpiRSgjXDQQVcQCnVm1argr/KkpcHzz8Pf/+6dDx061LtSLyIthwK0ERITO5GR8QOKiqZRVrYy8O2ZeR2bPv0UhgzxutlfdBEUFwe+aREJgwK0kTIzfww4Vq/+c7Nts39/mDkTfvtbeOEFyMnxvotIbClAGyk5+RB69PgOa9ZMYs+e5usAkpgIP/85/N//QXKy97z58eNhd3TblYpIIyhAI+ANrC9h3bq/Nfu2R4yAOXPgqqvgj3/0HhOyaFGzlyEiKEAj0qXLMXTqdDSrVz+Ec81/32VqKkyaBP/6F6xaBcOGea3xAmhbKiL1UIBGqG/fm9i58ys2bXotZjWcfTZ89hkcd5z3pM8zzoD162NWjkjcUYBGKD39O7Rv3zfwgfUNOeggmD4d/vQneO8972r9v/8d05JE4oYCNEIJCYn06fNjiotnsm1bbG9cN4Prr/du/ezdG846C665BrZvj2lZIm2eArQJMjJ+QEJCR5Ytu5XduzfGuhwGDYKPP4ZbbvHOkQ4fDgUFsa5KpO1SgDZBUlJXBgy4l+LimcyaNZC1a58O5AF0jdG+Pdx3H7zzjrcHOnKk7qcXCYoCtIkyM69n+PA5pKQM5Isvvs+8eaPZvn1hrMtizJgD76f/+utYVyXStihAoyA1dQh5ee9z+OFPsH37QmbPzmXZstupqNgR07qq7qefPNm7nz4vzwtVEYkOBWiUmCWQkXElRx31Bb16XcLKlfcya9aRbNwYu2FOXl1w6aUwdy507Agnn+w9BVREmk4BGmXt2qUzcOBT5Ob+l1CoIwsWnMmCBedRVrYqpnUdeii89ZbXZ/Skk2DNmpiWI9ImBBagZtbXzGaY2SIzW2hmP/GndzOzt81sif+e5k83M3vYzL4ys/lmNiyo2ppD167Hkp8/l/79f8/mzW8wa9YRrFr1QKCPBGnIEUfAf/4DRUXw7W/D5s0xK0WkTQhyD3QPcLNz7ghgJHCdmR0J3Aa865zLBt71vwOcCmT7r6uBRwOsrVkkJLTjkENuY8SIz+na9TiWLr2ZgoJ8Sko+illNI0Z4t4B++aV355LGiopELrAAdc6tdc7N8T9vAxYBfYCzgcn+YpOBc/zPZwN/d56PgK5mlhFUfc2pQ4d+DBnybwYNeok9ezYxd+43+OKLH1JeHptdwBNO8C4uffyxd5VeHZ1EItMs50DNrB+QB3wM9HLOrQUvZIGe/mJ9gJonCgv9aW2CmdGjx7mMGPE5mZk3sXbtk8yaNZB1656JydjRc8/1HmD31lte02aNExVpvMAD1MxSgReBG51zW+tbtJZpBySLmV1tZrPNbHZRUVG0ymw2iYmdyMq6n/z8Ajp0OJTFiy/l00/HsH374mav5fvf9wbdT53qNSNRNyeRxgk0QM0sCS88pzjnXvInr686NPffN/jTC4G+NX6eCRxwrdg5N8k5l++cy+/Ro0dwxQcsNXUoeXn/j8MOe4zS0nnMnp3D8uW/pKJiZ7PWccstcNtt8Nhj8MtfNuumRVq9IK/CG/AksMg590CNWa8Cl/mfLwNeqTH9Uv9q/EigpOpQv60yS6B376s56qgv6NlzLF9//Vs++WQwmza90ax1/O53cPXVcM898GBsm0uJtCpB7oGOAi4BxpjZPP91GnAvcJKZLQFO8r8DTAeWAV8BjwM/CrC2FqVdu54cccTfGTr0PcyS+OyzU1m48AJ2726e5p5mXkPm88+Hn/7Uu3NszJUVAAARg0lEQVRJRBpmsW5+0RT5+flu9uzZsS4jqiord7Fy5X18/fVvCYU6kpU1kV69xuHt0Adr1y4480yvr+iLL3oNm0XikZkVOOfyG1pOdyK1MAkJ7enX7xfk588jJWUgixdfymefnd4sdzK1bw8vvQT5+XDBBXryp0hDFKAtVMeOA8nLe5+srIcoLv4vn3wyiNWr/xr4M5hSU+H1171bP886S/1EReqjAG3BzEJkZt7AiBEL6Nz5aJYsuZZ588awY8dXgW63e3dvfGj37nDKKbC4+UdYibQKCtBWoEOH/uTkvMXhhz/hD3kawsqVE3AuuNHvffp4IZqQ4HVwWrkysE2JtFoK0FbCzPx2eZ+TlnYyy5bdypw5x1BauiCwbWZnw5tvQkmJF6Kt8L4FkUApQFuZ9u17M3jwvzjyyOcpK1tOQcEwVqz4FZWVwdzQnpsLr73mdbM/9VTYWt+9ZCJxRgHaCpkZPXtewIgRn9Ojx3dZseJuCgry2br1k0C2961vwbRpXlf7c86BsrJANiPS6ihAW7F27Xpw5JFTGDz4VcrLNzFnzkiWLv1ZILeDnn66N8B+xgy48EKvMbNIvFOAtgHp6WcyYsRCMjKuZNWq+5g9O4fi4vejvp2LL4aHH/b6iV59tZqPiChA24ikpK4cfvgkhg59F+cqmDdvNF9++SP27InuScsf/xjuuguefhpuvVUhKvFNAdrGpKWNYcSIz8jMvJE1a/4aSHOSu+6C66+H+++He+9teHmRtkoB2gZ599A/SF7e/yMUSuWzz05l0aLLotYB3wweesg7pL/jDrjsMtiyJSqrFmlVFKBtWJcux5CfP5eDD/4569dP4ZNPBrFly8yorDshwTuM/+UvYcoUGDzYe2CdSDxRgLZxCQntGTDgtwwf/gmhUGc+/fQEVq78Q1QeI5KUBL/+tfdspbQ0OO00uOoqb+C9SDxQgMaJTp3yGD78E3r0OI9ly25jwYJzKS8vjsq6hw/3mo7cfru3VzpkCLz9dlRWLdKiKUDjSGJiZ448ciqHHvogmze/TkFBPtu2zYvKutu39zrb/9//QceO3q2f11wD27ZFZfUiLZICNM6YGX373khu7kwqK3cyd+4xrF37VNTWf/TRMGeO96ylSZO8vdH33ova6kVaFAVonOrSZRT5+XPp3PkbfPHFlSxefGXU7mDq0MF72uf//gft2nnPob/+eti+PSqrF2kxFKBxrF27ngwd+hYHH/xz1q17irlzv8HOnUujtv5Ro7z753/yE/jLXyAnxwtVkbZCARrnzEIMGPBbBg/+N2VlK5g9ezgbN77S8A/DlJICEyfufTzI6NFw002wY0fUNiESMwpQASA9/QyGD59Dhw6HsmDBOSxdehuVldHrGDJ6NHz6KfzoR16g5uZ6F5xEWjMFqFTr0KE/eXn/j4yMq1m16g/Mn38Su3ati9r6U1Phz3+Gd9+F3bu9Nnk/+5na40nrpQCVfYRCyRx++GMMHPg3tm79mIKCYRQXR/fE5ZgxMH++N+j+vvsgLw9mzYrqJkSahQJUanXQQZcxbNhHhEIdmTfveFatuj8qdy9V6dwZHnvMe2RIaSkcc4x3X/2uXVHbhEjgFKBSp9TUHIYPn016+tksXXoLCxeez5490b1P8+STYcECuPxy+P3vvWfS61HK0looQKVeiYldGDRoGoceOoGNG1+hoCCf0tL5Ud1Gly7w5JPes5c2bfIG4595JvzjH97eqUhLpQCVBnl3L91Mbu4MKiq2M2fOSNatmxz17Zx+OixcCDff7I0fvfhi6NXLe4TIv//tXXgSaUkUoBK2rl2/xfDhc+jc+WgWL76cL774IRUV0b2EnpYGf/iD9xTQ99+HSy/1GpOcdRYcdJD3KJEZM6CiIqqbFYmIRfPCQHPLz893s2fPjnUZcaeycg/Ll/+CVav+QGpqLn37/oz09LMJhVIC2V55uReizz0HL7/s3RLauzdccAFcdJHXDcoskE1LnDKzAudcfoPLKUAlUhs3vsqSJdeza9cqQqFU0tO/Q69e40hLOx6zUCDb3LHDO1f6j3/A9OleuGZleUF64YUwcGAgm5U4owCVZuFcJSUl/2PdumcoKnqBioqttGvXm169LqJXr3Gkpg4NbNtbtsBLL3lhOmOG94C7vDwvSMeOhb59A9u0tHEKUGl2FRVlbNr0b9avf5bNm6fj3B46dhxCr17j6NnzIpKTMwPb9po1MHWqd5hfNSj/W9/y9kzPPx/S0wPbtLRBClCJqd27N1JUNJX1659l69YPAaNr1+Po1esSevT4DomJnQPb9ldfwfPPe3umixZBYiKcdJLXVm/YMO/VpUtgm5c2QAEqLcbOnUtZv/5Z1q9/lp07vyIhIZnu3c+iV69L6Nbt2yQkJAWyXee8W0afe87bO12+fO+8rCwvSIcP9155edCtWyBlSCukAJUWxznHtm2zWLfuGTZseJ49ezaRlJROz55j6dVrHJ06HYUFeDl9wwavW/6cOd7dTgUF3nCpKv37e2FaM1i7dw+sHGnBFKDSolVWlrN585usX/8Mmza9SmVlGR06ZFefL01JyWqWOjZt2huoVe/Llu2df/DBe8O0Klh79myW0iSGFKDSauzZU0JR0UusX/8MxcUzAUdSUg86dhzsvwbRseNgUlIGkZTUNfB6tmyBuXP37qXOmQNLluydn5m5N0wHDPAuUKWne3ur6ele2z6NS23dFKDSKpWVrWLjxn+xfft8tm9fwPbtC6io2HtDfPv2mdVhujdgjyAU6hhoXSUlXqjWPPz/8kvvPOv+2rXbG6r1vaoCNz3d69wvLUfMA9TMngLOADY45wb70+4GfgAU+Yvd4Zyb7s+7HbgSqABucM692dA2FKBtn3OOXbtW+mG6sDpUt2//HOeqet8Zycn9awTqYD9kDyMhoX1gtZWWwtq1sHHjga9Nmw6ctnlz7YEL3oP4qsK0WzdvlEDXrnW/1/zcuTOEgrlvIW61hAA9FigF/r5fgJY65ybst+yRwHPAUUBv4B3gMOdcvXc8K0Djl3MV7Ny5dL9QXcDOnV/iXNWjSEKkpBxWfRqgfftMEhO7k5SUXuOVFthdU/urqPBOD9QXtkVF3jIlJVBc7L2H05GqU6f6Qzc11dszTkqq/b2+eQ39pi0KN0ATgyrAOfe+mfULc/Gzgeedt0ux3My+wgvTDwMqT1o5My8cU1IOo0ePc6unV1buZseOL/cJ1W3b5lBUNA2obWfBSEzstl+o7v/aN3QTE7tg1vg+PKHQ3r3MxtizB7Zu9QK1KlTrey8u9vaMFy3aOy2o5iuhECQn73116FD753DntWvnrbey0ttbd65pn6u+f/e7cMgh0f/zBxag9bjezC4FZgM3O+e2AH2Aj2osU+hPE2mUhIR2pKYOJjV18D7TKyp2Ul5eRHn5RsrLN/nvB77KypazbdsnlJdvxLm6+ueFqkM1FOpEKNSBhATvFQqlVH/2vld9Ttlnuf2XrblcQkI7zEKYJQIJJCYa3bpFPk7VOa/Tf3m599q9O/z3+ubt2uW9yspg507vff/PpaXeXnVt85rz6QNDh7aNAH0U+A3ersBvgPuB7wO1XbOs9dyCmV0NXA1w8MEHB1OltDmhUAdCoYNJTg7vvxnnHBUVpfWGbXn5RioqSqms3EF5eREVFTuprNxJZeWO6s91h3Cjqq8O1P3fG55Xc3rtr8TERJKSEunYse5lDnxVbcP8vXEDEvxxvAn7TK9rPiRQXm6Ul+/7XjWCwcxhhv/a+xkcCQm1LeN996bvOy89/VigVxT+LfbVrAHqnFtf9dnMHgde878WAjVbP2QCa+pYxyRgEnjnQIOpVOKdmZGY2InExE506NAv4vU4V0FlZRkVFTv8cN1Za9B603dUh65zFTi3x3+v8M/r1px24Ls3v65l9uDcbiord9T43rhXa5aa+jYpKa08QM0swzm31v96LrDA//wq8A8zewDvIlI2oOc0SqtnFiIU6hj4MKugeRebK2sJVod3sFhZvQw4nNv3vXHzax6Q7v28711qdX2u/Tft2wdw/E6AAWpmzwHHAelmVgjcBRxnZrl4f2MrgB8COOcWmtlU4HNgD3BdQ1fgRaT5eEEU8kcsBDc0rLXRQHoRkf2EO4xJz0QSEYmQAlREJEIKUBGRCClARUQipAAVEYmQAlREJEIKUBGRCClARUQipAAVEYlQq74TycyKgK8bXDB46cDGWBcRhtZSJ7SeWlVndLWUOg9xzvVoaKFWHaAthZnNDue2r1hrLXVC66lVdUZXa6mzig7hRUQipAAVEYmQAjQ6JsW6gDC1ljqh9dSqOqOrtdQJ6ByoiEjEtAcqIhIhBaiISIQUoE1gZn3NbIaZLTKzhWb2k1jXVB8zC5nZXDN7reGlY8PMuprZNDNb7P+9HhPrmmpjZjf5/+YLzOw5M0uOdU1VzOwpM9tgZgtqTOtmZm+b2RL/PS2WNfo11Vbnff6//Xwze9nMusayxoYoQJtmD96z7Y8ARgLXmdmRMa6pPj8BFsW6iAY8BLzhnBsIDKUF1mtmfYAbgHzn3GAgBIyNbVX7+Btwyn7TbgPedc5lA+/632PtbxxY59vAYOdcDvAlcHtzF9UYCtAmcM6tdc7N8T9vw/ufvU9sq6qdmWUCpwNPxLqWuphZZ+BY4EkA59xu51xxbKuqUyLQwbyHo6dQx2O4Y8E59z6web/JZwOT/c+TgXOataha1Fanc+4tt/cZyh/hPeK8xVKARomZ9QPygI9jW0mdJgI/w3tubEs1ACgCnvZPNTxhZi3uecDOudXABGAlsBYocc69FduqGtSr6pHi/nvPGNcTju8D/4l1EfVRgEaBmaUCLwI3Oue2xrqe/ZnZGcAG51xBrGtpQCIwDHjUOZcHbKdlHGruwz9/eDbQH+gNdDSzcbGtqm0xs5/jnSKbEuta6qMAbSIzS8ILzynOuZdiXU8dRgFnmdkK4HlgjJk9G9uSalUIFDrnqvbip+EFaktzIrDcOVfknCsHXgK+EeOaGrLezDIA/PcNMa6nTmZ2GXAGcLFr4QPVFaBNYGaGd75ukXPugVjXUxfn3O3OuUznXD+8ix3vOeda3B6Tc24dsMrMDvcnnQB8HsOS6rISGGlmKf5/AyfQAi927edV4DL/82XAKzGspU5mdgowHjjLObcj1vU0RAHaNKOAS/D26Ob5r9NiXVQr92NgipnNB3KB38W4ngP4e8jTgDnAZ3j/H7WYWxDN7DngQ+BwMys0syuBe4GTzGwJcJL/PabqqPPPQCfgbf//p7/GtMgG6FZOEZEIaQ9URCRCClARkQgpQEVEIqQAFRGJkAJURCRCClCJa2Z2XEvuTiUtmwJURCRCClBpFcxsnJnN8gdXP+b3Ni01s/vNbI6ZvWtmPfxlc83soxo9JdP86Vlm9o6Zfer/5lB/9ak1epBO8e8uwszuNbPP/fVMiNEfXVowBai0eGZ2BHABMMo5lwtUABcDHYE5zrlhwH+Bu/yf/B0Y7/eU/KzG9CnAX5xzQ/HuXV/rT88DbgSOxOsINcrMugHnAoP89fw22D+ltEYKUGkNTgCGA5+Y2Tz/+wC81nz/9Jd5FvimmXUBujrn/utPnwwca2adgD7OuZcBnHNlNe61nuWcK3TOVQLzgH7AVqAMeMLMzgNa/H3Z0vwUoNIaGDDZOZfrvw53zt1dy3L13Zds9czbVeNzBZDoN/U9Cq/T1jnAG42sWeKAAlRag3eB882sJ1Q/3+cQvP9+z/eXuQj4wDlXAmwxs2/50y8B/uv3aS00s3P8dbQ3s5S6Nuj3eO3inJuOd3ifG8QfTFq3xFgXINIQ59znZvYL4C0zSwDKgevwGi4PMrMCoATvPCl47dr+6gfkMuAKf/olwGNm9mt/Hd+tZ7OdgFf8h8UZcFOU/1jSBqgbk7RaZlbqnEuNdR0Sv3QILyISIe2BiohESHugIiIRUoCKiERIASoiEiEFqIhIhBSgIiIR+v/vayjK54hu3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x260ef2acc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Obtained losses from the saved model log\n",
    "epochs = np.arange(1,16)\n",
    "training_loss = np.array([411.878,254.151,222.337,206.397,181.998,166.810,158.918,154.449,152.692,151.322,150.832,150.105,150.222])\n",
    "validation_loss = np.array([260.459,209.322,195.402,180.682,163.290,155.147,151.287,149.529,148.718,148.415,148.064,147.929,147.959])\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epoch,training_loss,color='b',label='training loss')\n",
    "plt.plot(epoch,validation_loss,color='y',label='validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
