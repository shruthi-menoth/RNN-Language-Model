{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_id.pickle', 'rb') as handle:\n",
    "    word_to_id = pickle.load(handle)\n",
    "\n",
    "with open('id_to_word.pickle', 'rb') as handle:\n",
    "    id_to_word = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    vocab_size = 9999\n",
    "    batch_size = 20\n",
    "    num_steps = 20  # sequence length\n",
    "    hidden_size = 200  # number of hidden units in LSTM; \n",
    "    keep_prob = 0.5  # 1 - dropoff rate\n",
    "    num_layers = 2  # number of LSTM layers\n",
    "    max_grad_norm = 5  # max gradient \n",
    "    init_scale = 0.1  # the initial scale of the weights\n",
    "    max_epoch = 4  # the number of epochs trained with the initial learning rate\n",
    "    max_max_epoch = 13  # the total number of epochs for training\n",
    "    learning_rate = 1.0  # the initial value of the learning rate\n",
    "    lr_decay = 0.5  # the decay of the learning rate for each epoch after \"max_epoch\"\n",
    "\n",
    "eval_config = config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    \"\"\"The PTB model.\"\"\"\n",
    "\n",
    "    def __init__(self, is_training, config, input_=None):\n",
    "        batch_size = config.batch_size\n",
    "        num_steps = config.num_steps\n",
    "        hidden_size = config.hidden_size\n",
    "        vocab_size = config.vocab_size\n",
    "        \n",
    "        if input_ is not None:\n",
    "            # For normal training and validation\n",
    "            self._input = input_\n",
    "            self._input_data = input_.input_data\n",
    "            self._targets = input_.targets\n",
    "            \n",
    "        else:\n",
    "            # For text generations\n",
    "            self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "            self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "        def lstm_cell():\n",
    "\n",
    "            if 'reuse' in inspect.getargspec(\n",
    "                    tf.contrib.rnn.BasicLSTMCell.__init__).args:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True,\n",
    "                    reuse=tf.get_variable_scope().reuse)\n",
    "            else:\n",
    "                return tf.contrib.rnn.BasicLSTMCell(\n",
    "                    hidden_size,\n",
    "                    forget_bias=0.0,\n",
    "                    state_is_tuple=True)\n",
    "            \n",
    "\n",
    "    \n",
    "        attn_cell = lstm_cell\n",
    "\n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "\n",
    "            def attn_cell():\n",
    "                return tf.contrib.rnn.DropoutWrapper(\n",
    "                    lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "\n",
    "        # Stacking multiple LSTMs\n",
    "        attn_cells = [attn_cell() for _ in range(config.num_layers)]\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell(attn_cells, state_is_tuple=True)\n",
    "        \n",
    "\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, hidden_size], dtype=tf.float32)\n",
    "            input_embeddings = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "            # The shape of `input_embeddings` is [batch_size, num_steps, hidden_size]\n",
    "        \n",
    "        # Implement dropoff (for training only)\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            input_embeddings = tf.nn.dropout(input_embeddings, config.keep_prob)\n",
    "\n",
    "\n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        \n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                (cell_output, state) = stacked_lstm(input_embeddings[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "        \n",
    "        output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, hidden_size])\n",
    "        \n",
    "        # Compute logits\n",
    "        softmax_w = tf.get_variable(\n",
    "            \"softmax_w\", [hidden_size, vocab_size], dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\n",
    "            \"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "        \n",
    "        self._logits = logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "        \n",
    "        # Sample based on the size of logits (used for text generation)\n",
    "        self._logits_sample = tf.multinomial(logits, 1)\n",
    "        \n",
    "        # Reshape logits to be 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "\n",
    "\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,  # shape: [batch_size, num_steps, vocab_size]\n",
    "            self._targets,  # shape: [batch_size, num_steps]\n",
    "            tf.ones([batch_size, num_steps], dtype=tf.float32),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "\n",
    "        # Update the cost variables\n",
    "        self._cost = cost = tf.reduce_sum(loss)\n",
    "        self._final_state = state\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # Optimizer\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars),\n",
    "            global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        self._new_lr = tf.placeholder(\n",
    "            tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "        \n",
    "        \n",
    "    # To update learning rate\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "    \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    \n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "    \n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def logits_sample(self):\n",
    "        return self._logits_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0)Word to be predicted:greater\n",
      "1)Word to be predicted:economy\n",
      "2)Word to be predicted:to\n",
      "3)Word to be predicted:he\n",
      "4)Word to be predicted:a\n"
     ]
    }
   ],
   "source": [
    "import reader \n",
    "import random\n",
    "k = reader._read_words(\"data/ptb.test.txt\")\n",
    "x = random.sample(range(1, len(k)), 5)\n",
    "for i in range(5):\n",
    "    print(str(i)+')'+ 'Word to be predicted:' + str(k[x[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Insert starting word\n",
    "feed = np.array(word_to_id['a']).reshape(1, 1)\n",
    "text_length = 20\n",
    "\n",
    "def generate_text(session, model, feed, text_length):\n",
    "    state = session.run(model.initial_state)\n",
    "    fetches = {\n",
    "        \"final_state\": model.final_state,\n",
    "        \"logits\": model.logits_sample\n",
    "    }\n",
    "    \n",
    "    generated_text = [feed]\n",
    "    \n",
    "    for i in range(text_length):\n",
    "        feed_dict = {}\n",
    "        feed_dict[model.input_data] = feed\n",
    "        \n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        \n",
    "        vals = session.run(fetches, feed_dict)\n",
    "\n",
    "        state = vals[\"final_state\"]\n",
    "        feed = vals[\"logits\"]\n",
    "        \n",
    "        \n",
    "        generated_text.append(feed)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'assignment2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\Anaconda3\\envs\\cs231n\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from assignment2\\model.ckpt\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path assignment2\\model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Restoring parameters from assignment2\\model.ckpt\n",
      "Model restored from file: assignment2\\model.ckpt\n",
      "\n",
      "Word to be predicted : a\n",
      "Generated text: a new orleans ship at the lead of it to help the <unk> but in the business <unk> disrupted <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "    \n",
    "    # Define model for text generations\n",
    "    with tf.name_scope(\"Feed\"):\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            mfeed = PTBModel(is_training=False, config=eval_config)\n",
    "    \n",
    "    sv = tf.train.Supervisor(logdir=model_path)\n",
    "    with sv.managed_session() as session:\n",
    "       \n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        sv.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored from file: %s\\n\" % ckpt.model_checkpoint_path)\n",
    "        \n",
    "        generated_text = generate_text(session, mfeed, np.array(feed).reshape(1, 1), text_length)\n",
    "        generated_text = ' '.join([id_to_word[text[0, 0]] for text in generated_text])\n",
    "        print('Word to be predicted : a')\n",
    "        print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
